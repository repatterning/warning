"""Module points.py"""
import logging
import os

import dask
import pandas as pd

import config
import src.elements.partitions as prt
import src.functions.directories
import src.functions.objects
import src.functions.streams


class Points:
    """
    <b>Notes</b><br>
    ------<br>

    The time series data.
    """

    def __init__(self):
        """
        Constructor
        """

        self.__configurations = config.Config()

        # An instance for reading & writing JSON (JavaScript Object Notation) objects, CSV, ...
        self.__objects = src.functions.objects.Objects()
        self.__streams = src.functions.streams.Streams()
        self.__directories = src.functions.directories.Directories()

        # The uniform resource locator, data columns, etc.
        self.__url = ('https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0'
                      '&request=getTimeseriesValues&ts_id={ts_id}'
                      f'&period={self.__configurations.period}'
                      '&from={datestr}&returnfields=Timestamp,Value,Quality Code&metadata=true'
                      '&md_returnfields=ts_id,ts_name,ts_unitname,ts_unitsymbol,station_id,'
                      'catchment_id,parametertype_id,parametertype_name,river_name&dateformat=UNIX&format=json')

        self.__rename = {'Timestamp': 'timestamp', 'Value': 'value', 'Quality Code': 'quality_code'}

    @dask.delayed
    def __get_data(self, url: str):
        """

        :param url:
        :return:
        """

        parts = self.__objects.api(url=url)

        # The data in data frame form
        columns = parts[0]['columns'].split(',')
        frame = pd.DataFrame.from_records(data=parts[0]['data'], columns=columns)
        frame.rename(columns=self.__rename, inplace=True)

        # The identification codes of the time series
        frame = frame.assign(
            station_id=parts[0]['station_id'], catchment_id=parts[0]['catchment_id'])

        return frame

    @dask.delayed
    def __extra_features(self, data: pd.DataFrame, partition: prt.Partitions):
        """

        :param data:
        :param partition:
        :return:
        """

        data = data.assign(
            catchment_size=partition.catchment_size, gauge_datum=partition.gauge_datum, on_river=int(partition.on_river))

        return data

    @dask.delayed
    def __persist(self, data: pd.DataFrame, partition: prt.Partitions) -> str:
        """

        :param data:
        :param partition:
        :return:
        """

        directory = os.path.join(self.__configurations.series_, str(partition.catchment_id), str(partition.ts_id))
        self.__directories.create(path=directory)

        message = self.__streams.write(
            blob=data, path=os.path.join(directory, f'{partition.datestr}.csv'))

        return message

    def exc(self, partitions: list[prt.Partitions]):
        """

        :param partitions: ts_id, datestr, catchment_size, gauge_datum, on_river
        :return:
        """

        computations = []
        for partition in partitions:

            url = self.__url.format(ts_id=partition.ts_id, datestr=partition.datestr)
            data = self.__get_data(url=url)
            data = self.__extra_features(data=data.copy(), partition=partition)
            message = self.__persist(data=data, partition=partition)

            computations.append(message)
        calculations = dask.compute(computations, scheduler='threads')[0]

        logging.info(calculations)
